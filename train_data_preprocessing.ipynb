{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/jeremychua/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jeremychua/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/jeremychua/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "from string import punctuation\n",
    "\n",
    "import nltk\n",
    "from nltk import ngrams\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/test/preprocess_output.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Get contextual features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['word_count'] = df['title'].str.split().str.len()\n",
    "df['character_count'] = df['title'].str.len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "# Function to generate n-grams for the title\n",
    "def generate_ngrams(text, n):\n",
    "    # Tokenize the text into words\n",
    "    tokens = word_tokenize(text)\n",
    "    # Generate n-grams\n",
    "    return list(ngrams(tokens, n))\n",
    "\n",
    "df['bigrams'] = df['title'].apply(lambda title: generate_ngrams(title, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contextual_features(title):\n",
    "    doc = nlp(title)\n",
    "    lemma = []\n",
    "    pos = []\n",
    "    tag = []\n",
    "    dep = []\n",
    "    label = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if token.text in punctuation:\n",
    "            continue\n",
    "        lemma.append(token.lemma_)\n",
    "        pos.append(token.pos_)\n",
    "        tag.append(token.tag_)\n",
    "        dep.append(token.dep_)\n",
    "        label.append(token.ent_type_)\n",
    "        \n",
    "    return lemma, pos, tag, dep, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = []\n",
    "pos = []\n",
    "tag = []\n",
    "dep = []\n",
    "label = []\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    title = row['title']\n",
    "    l, p, t, d, la = get_contextual_features(title)\n",
    "    lemma.append(l)\n",
    "    pos.append(p)\n",
    "    tag.append(t)\n",
    "    dep.append(d)\n",
    "    label.append(la)\n",
    "\n",
    "df['lemma'] = lemma\n",
    "df['pos'] = pos\n",
    "df['tag'] = tag\n",
    "df['dep'] = dep\n",
    "df['label'] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Get trigger words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_digit(word):\n",
    "    for char in word:\n",
    "        if char.isdigit():\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def extract_trigger_words(title):\n",
    "    result = []\n",
    "    pos_tag = ['ADJ', 'NOUN', 'VERB', 'ADV', 'NNP', 'PROPN'] \n",
    "    # tag_ls = ['NN', 'NNP', 'NNPS']\n",
    "    label_type = ['TIME', 'PERCENT', 'MONEY', 'QUANTITY', 'ORDINAL', 'CARDINAL']\n",
    "    doc = nlp(title[0].lower()+title[1:])\n",
    "\n",
    "    prev_label = None\n",
    "    trigger_combi = None\n",
    "    for token in doc:\n",
    "        # print(token.text, token.ent_type_, token.pos_, token.tag_)\n",
    "        if token.text in nlp.Defaults.stop_words or token.text in punctuation or contains_digit(token.text):\n",
    "            continue\n",
    "        if (token.pos_ in pos_tag) and (token.ent_type_ not in label_type): # or (token.tag_ in tag_ls)\n",
    "            if prev_label:\n",
    "                if prev_label == token.ent_type_:\n",
    "                    trigger_combi += '_' + token.text\n",
    "                    continue\n",
    "                else:\n",
    "                    if trigger_combi:\n",
    "                        result.append(trigger_combi.lower())\n",
    "                        trigger_combi = None\n",
    "                        prev_label = None\n",
    "                    result.append(token.text.lower())\n",
    "                    continue\n",
    "            \n",
    "            if len(token.ent_type_) > 0:\n",
    "                prev_label = token.ent_type_\n",
    "                trigger_combi = token.text\n",
    "            else:\n",
    "                if trigger_combi:\n",
    "                    result.append(trigger_combi.lower())\n",
    "                    trigger_combi = None\n",
    "                    prev_label = None\n",
    "                result.append(token.text.lower())\n",
    "        \n",
    "    if trigger_combi:\n",
    "        result.append(trigger_combi.lower())\n",
    "        trigger_combi = None\n",
    "        prev_label = None\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asean_para_games']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_trigger_words(\"2019 ASEAN Para Games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['trigger_words'] = df['title'].apply(extract_trigger_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['trigger_words'].map(len) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Get TF-IDF of trigger words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(df['title'])\n",
    "words = vectorizer.get_feature_names_out()\n",
    "tfidf_scores = tfidf_matrix.sum(axis=0).A1\n",
    "word_scores = dict(zip(words, tfidf_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['context_score'] = None\n",
    "for index, row in df.iterrows():\n",
    "    score = {}\n",
    "    for word in row['trigger_words']:\n",
    "        if word in word_scores:\n",
    "            score[word] = word_scores[word]\n",
    "        else:\n",
    "            score[word] = 0\n",
    "            \n",
    "    if len(score) > 1:\n",
    "        max_score = max(score.values())\n",
    "        if max_score > 0:\n",
    "            for key in score:\n",
    "                score[key] = score[key] / max_score\n",
    "    \n",
    "    elif len(score) == 1:\n",
    "        for key in score:\n",
    "            score[key] = 1.0\n",
    "        \n",
    "    score = {k: v for k, v in score.items() if v > 0.5}\n",
    "    \n",
    "    df.at[index, 'context_score'] = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['context_score'].map(len) > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/test/output_with_context_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Get category of trigger words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_relatedness(word1, word2):\n",
    "    # Process the words with spaCy\n",
    "    token1 = nlp(word1)\n",
    "    token2 = nlp(word2)\n",
    "\n",
    "    max_similarity = 0\n",
    "    \n",
    "    # Iterate through all tokens of each word\n",
    "    for t1 in token1:\n",
    "        for t2 in token2:\n",
    "            similarity = t1.similarity(t2)\n",
    "            if similarity > max_similarity:\n",
    "                max_similarity = similarity\n",
    "    \n",
    "    return max_similarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23937943577766418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dh/hgl_kbnx491bp562snpkpz3r0000gn/T/ipykernel_60110/3344422160.py:11: UserWarning: [W008] Evaluating Token.similarity based on empty vectors.\n",
      "  similarity = t1.similarity(t2)\n"
     ]
    }
   ],
   "source": [
    "word1 = words_relatedness('asean', 'sports')\n",
    "word2 = words_relatedness('para', 'sports')\n",
    "word3 = words_relatedness('games', 'sports')\n",
    "\n",
    "avg = (word1 + word2 + word3) / 3\n",
    "print(avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2699144184589386\n",
      "0.16935458779335022\n"
     ]
    }
   ],
   "source": [
    "print(words_relatedness('earthquake', 'environment'))\n",
    "print(words_relatedness('earthquake', 'health'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_representative_word(words_to_check):\n",
    "    common_hypernyms = Counter()\n",
    "\n",
    "    for word in words_to_check:\n",
    "        synsets = wordnet.synsets(word)\n",
    "        for synset in synsets:\n",
    "            common_hypernyms.update(synset.hypernyms())\n",
    "\n",
    "    if not common_hypernyms:\n",
    "        return None\n",
    "    \n",
    "    return common_hypernyms.most_common(1)[0][0].lemma_names()[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "context_threshold = 0.7\n",
    "\n",
    "def get_category_row(current_categories, df_row):\n",
    "    print(df_row['title'])\n",
    "    cat_word_scores = {}\n",
    "    scores = {word:score for word, score in df_row['context_score'].items() if score > context_threshold}\n",
    "    for category in current_categories:\n",
    "        for word in scores.keys():\n",
    "            if len(word.split(\"_\")) > 1:\n",
    "                subwords = word.split(\"_\")\n",
    "                for subword in subwords:\n",
    "                    cat_word_scores[(category, subword)] = words_relatedness(subword, category)\n",
    "            else:\n",
    "                cat_word_scores[(category, word)] = words_relatedness(word, category)\n",
    "    \n",
    "    print(sorted(cat_word_scores.items(), key=lambda x: x[1], reverse=True))\n",
    "    max_score = max(cat_word_scores.values())\n",
    "    new_category = [k[0] for k, v in cat_word_scores.items() if v == max_score][0]\n",
    "    return current_categories, new_category\n",
    "\n",
    "\n",
    "def get_category_df(df, current_categories):\n",
    "    df['category'] = None\n",
    "    for idx, row in df.iterrows():\n",
    "        try:\n",
    "            current_categories, category = get_category_row(current_categories, row)\n",
    "            df.at[idx, 'category'] = category\n",
    "        except Exception as e:\n",
    "            print(\"Error at %d: %s\" % (idx, e))\n",
    "            print(current_categories)\n",
    "            break\n",
    "    \n",
    "    return df, current_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"business\", \"politics\", \"technology\", \"entertainment\", \"sports\", \"lifestyle\", \"health\", \"science\", \"education\", \"editorial\", \"international\", \"environment\", \"crime\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Western Air Express Flight 7\n",
      "[(('international', 'western'), 0.4049071967601776), (('politics', 'western'), 0.3474322557449341), (('entertainment', 'western'), 0.30807703733444214), (('education', 'western'), 0.3065800964832306), (('environment', 'western'), 0.28821176290512085), (('science', 'western'), 0.266686350107193), (('sports', 'western'), 0.26235148310661316), (('technology', 'western'), 0.2398119568824768), (('lifestyle', 'western'), 0.23076753318309784), (('business', 'western'), 0.20046818256378174), (('editorial', 'western'), 0.1918196827173233), (('crime', 'western'), 0.1251412034034729), (('health', 'western'), 0.12266942858695984)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['business',\n",
       "  'politics',\n",
       "  'technology',\n",
       "  'entertainment',\n",
       "  'sports',\n",
       "  'lifestyle',\n",
       "  'health',\n",
       "  'science',\n",
       "  'education',\n",
       "  'editorial',\n",
       "  'international',\n",
       "  'environment',\n",
       "  'crime'],\n",
       " 'international')"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_category_row(categories, df.iloc[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df, current_categories \u001b[38;5;241m=\u001b[39m \u001b[43mget_category_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 19\u001b[0m, in \u001b[0;36mget_category_df\u001b[0;34m(df, current_categories)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, row \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 19\u001b[0m         current_categories, category \u001b[38;5;241m=\u001b[39m \u001b[43mget_category_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurrent_categories\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m         df\u001b[38;5;241m.\u001b[39mat[idx, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m category\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[0;32mIn[27], line 8\u001b[0m, in \u001b[0;36mget_category_row\u001b[0;34m(current_categories, df_row)\u001b[0m\n\u001b[1;32m      6\u001b[0m     scores \u001b[38;5;241m=\u001b[39m {word:score \u001b[38;5;28;01mfor\u001b[39;00m word, score \u001b[38;5;129;01min\u001b[39;00m df_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m score \u001b[38;5;241m>\u001b[39m context_threshold}\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m df_row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m----> 8\u001b[0m         cat_word_scores[(category, word)] \u001b[38;5;241m=\u001b[39m \u001b[43mwords_relatedness\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m max_score \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(cat_word_scores\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m     11\u001b[0m new_category \u001b[38;5;241m=\u001b[39m [k[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m cat_word_scores\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m v \u001b[38;5;241m==\u001b[39m max_score][\u001b[38;5;241m0\u001b[39m]\n",
      "Cell \u001b[0;32mIn[25], line 7\u001b[0m, in \u001b[0;36mwords_relatedness\u001b[0;34m(word1, word2)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m synset1 \u001b[38;5;129;01min\u001b[39;00m wordnet\u001b[38;5;241m.\u001b[39msynsets(word1):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m synset2 \u001b[38;5;129;01min\u001b[39;00m wordnet\u001b[38;5;241m.\u001b[39msynsets(word2):\n\u001b[0;32m----> 7\u001b[0m         similarity \u001b[38;5;241m=\u001b[39m \u001b[43msynset1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwup_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43msynset2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m similarity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m similarity \u001b[38;5;241m>\u001b[39m max_similarity:\n\u001b[1;32m      9\u001b[0m             max_similarity \u001b[38;5;241m=\u001b[39m similarity\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/three_nine/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:993\u001b[0m, in \u001b[0;36mSynset.wup_similarity\u001b[0;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# Note: No need for an additional add-one correction for non-nouns\u001b[39;00m\n\u001b[1;32m    982\u001b[0m \u001b[38;5;66;03m# to account for an imaginary root node because that is now\u001b[39;00m\n\u001b[1;32m    983\u001b[0m \u001b[38;5;66;03m# automatically handled by simulate_root\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[38;5;66;03m# subsuming.  Add this to the LCS path length to get the path\u001b[39;00m\n\u001b[1;32m    989\u001b[0m \u001b[38;5;66;03m# length from each synset to the root.\u001b[39;00m\n\u001b[1;32m    990\u001b[0m len1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortest_path_distance(\n\u001b[1;32m    991\u001b[0m     subsumer, simulate_root\u001b[38;5;241m=\u001b[39msimulate_root \u001b[38;5;129;01mand\u001b[39;00m need_root\n\u001b[1;32m    992\u001b[0m )\n\u001b[0;32m--> 993\u001b[0m len2 \u001b[38;5;241m=\u001b[39m \u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshortest_path_distance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    994\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubsumer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msimulate_root\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimulate_root\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mneed_root\u001b[49m\n\u001b[1;32m    995\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    996\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m len1 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m len2 \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    997\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/three_nine/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:827\u001b[0m, in \u001b[0;36mSynset.shortest_path_distance\u001b[0;34m(self, other, simulate_root)\u001b[0m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m==\u001b[39m other:\n\u001b[1;32m    825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 827\u001b[0m dist_dict1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shortest_hypernym_paths\u001b[49m\u001b[43m(\u001b[49m\u001b[43msimulate_root\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    828\u001b[0m dist_dict2 \u001b[38;5;241m=\u001b[39m other\u001b[38;5;241m.\u001b[39m_shortest_hypernym_paths(simulate_root)\n\u001b[1;32m    830\u001b[0m \u001b[38;5;66;03m# For each ancestor synset common to both subject synsets, find the\u001b[39;00m\n\u001b[1;32m    831\u001b[0m \u001b[38;5;66;03m# connecting path length. Return the shortest of these.\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/three_nine/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:799\u001b[0m, in \u001b[0;36mSynset._shortest_hypernym_paths\u001b[0;34m(self, simulate_root)\u001b[0m\n\u001b[1;32m    796\u001b[0m     path[s] \u001b[38;5;241m=\u001b[39m depth\n\u001b[1;32m    798\u001b[0m     depth \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 799\u001b[0m     \u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhyp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_hypernyms\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    800\u001b[0m     queue\u001b[38;5;241m.\u001b[39mextend((hyp, depth) \u001b[38;5;28;01mfor\u001b[39;00m hyp \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39m_instance_hypernyms())\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m simulate_root:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/three_nine/lib/python3.9/site-packages/nltk/corpus/reader/wordnet.py:799\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    796\u001b[0m     path[s] \u001b[38;5;241m=\u001b[39m depth\n\u001b[1;32m    798\u001b[0m     depth \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 799\u001b[0m     queue\u001b[38;5;241m.\u001b[39mextend((hyp, depth) \u001b[38;5;28;01mfor\u001b[39;00m hyp \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39m_hypernyms())\n\u001b[1;32m    800\u001b[0m     queue\u001b[38;5;241m.\u001b[39mextend((hyp, depth) \u001b[38;5;28;01mfor\u001b[39;00m hyp \u001b[38;5;129;01min\u001b[39;00m s\u001b[38;5;241m.\u001b[39m_instance_hypernyms())\n\u001b[1;32m    802\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m simulate_root:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df, current_categories = get_category_df(df, categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'word_count', 'character_count', 'bigrams', 'lemma', 'pos',\n",
       "       'tag', 'dep', 'label', 'trigger_words', 'context_score', 'category'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['title', 'word_count', 'character_count', 'bigrams', 'lemma', 'pos', 'tag', 'dep', 'label', 'context_score', 'trigger_words']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/test/output_with_category.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "three_nine",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
