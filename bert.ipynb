{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kenny/anaconda3/envs/tensorflow-env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = 'output'\n",
    "VAL_RATIO = 0.3\n",
    "DATA_COLUMNS = ['title', 'lemma', 'pos', 'tag', 'dep', 'label', 'trigger_words', 'context_score']\n",
    "LABEL_COLUMN = 'category'\n",
    "LABEL_LIST = []\n",
    "DATASET_DIR = 'data/test/output_with_category.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset headers title,lemma,pos,tag,dep,label,trigger_words,context_score,category\n",
    "from sklearn.base import defaultdict\n",
    "\n",
    "\n",
    "def load_dataset(file_location, top_categories=40):\n",
    "    # Count the occurrences of each category\n",
    "    category_counts = defaultdict(int)\n",
    "    with open(file_location, 'r') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            category = row[-1]  # Assuming the category is the last column\n",
    "            category_counts[category] += 1\n",
    "    \n",
    "    # Select the top 40 most frequent categories\n",
    "    top_categories = sorted(category_counts, key=category_counts.get, reverse=True)[:top_categories]\n",
    "\n",
    "    # Populate label list\n",
    "    LABEL_LIST.extend(top_categories)\n",
    "\n",
    "    # Read the CSV file into a DataFrame, filtering out rows with categories not in the top 40\n",
    "    df = pd.read_csv(file_location, usecols=DATA_COLUMNS.append(LABEL_COLUMN))\n",
    "    df = df[df[LABEL_COLUMN].isin(top_categories)]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = list(range(0, len(LABEL_LIST)))\n",
    "data_df = load_dataset(DATASET_DIR)\n",
    "train_data, test_data, train_labels, test_labels = train_test_split(data_df, data_df[LABEL_COLUMN], test_size=VAL_RATIO, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>tag</th>\n",
       "      <th>dep</th>\n",
       "      <th>label</th>\n",
       "      <th>trigger_words</th>\n",
       "      <th>context_score</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2006 Pangandaran earthquake and tsunami</td>\n",
       "      <td>['2006', 'Pangandaran', 'earthquake', 'and', '...</td>\n",
       "      <td>['NUM', 'PROPN', 'NOUN', 'CCONJ', 'NOUN']</td>\n",
       "      <td>['CD', 'NNP', 'NN', 'CC', 'NN']</td>\n",
       "      <td>['nummod', 'compound', 'ROOT', 'cc', 'conj']</td>\n",
       "      <td>['DATE', 'GPE', '', '', '']</td>\n",
       "      <td>['pangandaran', 'earthquake', 'tsunami']</td>\n",
       "      <td>{'earthquake': 1.0}</td>\n",
       "      <td>geological_phenomenon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Battle of Santa Clara (1927)</td>\n",
       "      <td>['battle', 'of', 'Santa', 'Clara', '1927']</td>\n",
       "      <td>['NOUN', 'ADP', 'PROPN', 'PROPN', 'NUM']</td>\n",
       "      <td>['NN', 'IN', 'NNP', 'NNP', 'CD']</td>\n",
       "      <td>['ROOT', 'prep', 'compound', 'pobj', 'npadvmod']</td>\n",
       "      <td>['', '', 'GPE', 'GPE', 'DATE']</td>\n",
       "      <td>['battle', 'santa', 'clara']</td>\n",
       "      <td>{'battle': 1.0}</td>\n",
       "      <td>military_action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siege of Pondicherry (1793)</td>\n",
       "      <td>['siege', 'of', 'Pondicherry', '1793']</td>\n",
       "      <td>['NOUN', 'ADP', 'PROPN', 'NUM']</td>\n",
       "      <td>['NN', 'IN', 'NNP', 'CD']</td>\n",
       "      <td>['ROOT', 'prep', 'pobj', 'npadvmod']</td>\n",
       "      <td>['', '', '', 'DATE']</td>\n",
       "      <td>['siege', 'pondicherry']</td>\n",
       "      <td>{'siege': 1.0}</td>\n",
       "      <td>military_action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Battle of Leuthen</td>\n",
       "      <td>['battle', 'of', 'Leuthen']</td>\n",
       "      <td>['NOUN', 'ADP', 'PROPN']</td>\n",
       "      <td>['NN', 'IN', 'NNP']</td>\n",
       "      <td>['ROOT', 'prep', 'pobj']</td>\n",
       "      <td>['', '', '']</td>\n",
       "      <td>['battle', 'leuthen']</td>\n",
       "      <td>{'battle': 1.0}</td>\n",
       "      <td>military_action</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Glasgow St Enoch rail accident</td>\n",
       "      <td>['Glasgow', 'St', 'Enoch', 'rail', 'accident']</td>\n",
       "      <td>['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN']</td>\n",
       "      <td>['NNP', 'NNP', 'NNP', 'NN', 'NN']</td>\n",
       "      <td>['compound', 'compound', 'compound', 'compound...</td>\n",
       "      <td>['PERSON', 'PERSON', 'PERSON', '', '']</td>\n",
       "      <td>['glasgow', 'st', 'enoch', 'rail', 'accident']</td>\n",
       "      <td>{'rail': 1.0, 'accident': 0.7565370481203059}</td>\n",
       "      <td>bar</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     title  \\\n",
       "0  2006 Pangandaran earthquake and tsunami   \n",
       "1             Battle of Santa Clara (1927)   \n",
       "2              Siege of Pondicherry (1793)   \n",
       "3                        Battle of Leuthen   \n",
       "4           Glasgow St Enoch rail accident   \n",
       "\n",
       "                                               lemma  \\\n",
       "0  ['2006', 'Pangandaran', 'earthquake', 'and', '...   \n",
       "1         ['battle', 'of', 'Santa', 'Clara', '1927']   \n",
       "2             ['siege', 'of', 'Pondicherry', '1793']   \n",
       "3                        ['battle', 'of', 'Leuthen']   \n",
       "4     ['Glasgow', 'St', 'Enoch', 'rail', 'accident']   \n",
       "\n",
       "                                           pos  \\\n",
       "0    ['NUM', 'PROPN', 'NOUN', 'CCONJ', 'NOUN']   \n",
       "1     ['NOUN', 'ADP', 'PROPN', 'PROPN', 'NUM']   \n",
       "2              ['NOUN', 'ADP', 'PROPN', 'NUM']   \n",
       "3                     ['NOUN', 'ADP', 'PROPN']   \n",
       "4  ['PROPN', 'PROPN', 'PROPN', 'NOUN', 'NOUN']   \n",
       "\n",
       "                                 tag  \\\n",
       "0    ['CD', 'NNP', 'NN', 'CC', 'NN']   \n",
       "1   ['NN', 'IN', 'NNP', 'NNP', 'CD']   \n",
       "2          ['NN', 'IN', 'NNP', 'CD']   \n",
       "3                ['NN', 'IN', 'NNP']   \n",
       "4  ['NNP', 'NNP', 'NNP', 'NN', 'NN']   \n",
       "\n",
       "                                                 dep  \\\n",
       "0       ['nummod', 'compound', 'ROOT', 'cc', 'conj']   \n",
       "1   ['ROOT', 'prep', 'compound', 'pobj', 'npadvmod']   \n",
       "2               ['ROOT', 'prep', 'pobj', 'npadvmod']   \n",
       "3                           ['ROOT', 'prep', 'pobj']   \n",
       "4  ['compound', 'compound', 'compound', 'compound...   \n",
       "\n",
       "                                    label  \\\n",
       "0             ['DATE', 'GPE', '', '', '']   \n",
       "1          ['', '', 'GPE', 'GPE', 'DATE']   \n",
       "2                    ['', '', '', 'DATE']   \n",
       "3                            ['', '', '']   \n",
       "4  ['PERSON', 'PERSON', 'PERSON', '', '']   \n",
       "\n",
       "                                    trigger_words  \\\n",
       "0        ['pangandaran', 'earthquake', 'tsunami']   \n",
       "1                    ['battle', 'santa', 'clara']   \n",
       "2                        ['siege', 'pondicherry']   \n",
       "3                           ['battle', 'leuthen']   \n",
       "4  ['glasgow', 'st', 'enoch', 'rail', 'accident']   \n",
       "\n",
       "                                   context_score               category  \n",
       "0                            {'earthquake': 1.0}  geological_phenomenon  \n",
       "1                                {'battle': 1.0}        military_action  \n",
       "2                                 {'siege': 1.0}        military_action  \n",
       "3                                {'battle': 1.0}        military_action  \n",
       "4  {'rail': 1.0, 'accident': 0.7565370481203059}                    bar  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize data\n",
    "def tokenize_data(tokenizer, sentences, labels, max_length):\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                            sentence,\n",
    "                            add_special_tokens = True,\n",
    "                            max_length = max_length,\n",
    "                            pad_to_max_length = True,\n",
    "                            return_attention_mask = True,\n",
    "                            return_tensors = 'pt',\n",
    "                       )\n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    return TensorDataset(input_ids, attention_masks, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\", do_lower_case=True)\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(label_list), output_attentions=False, output_hidden_states=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and prepare data loaders\n",
    "train_dataset = tokenize_data(tokenizer, train_data, train_labels, max_length=128)\n",
    "val_dataset = tokenize_data(tokenizer, test_data, test_labels, max_length=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/kenny/anaconda3/envs/tensorflow-env/lib/python3.10/site-packages/tensorflow_text/core/pybinds/tflite_registrar.so, 0x0002): Symbol not found: __ZN4absl12lts_2023080210CHexEscapeENSt3__117basic_string_viewIcNS1_11char_traitsIcEEEE\n  Referenced from: <E5449F0F-CDE4-384F-B7F9-E7BD33907F26> /Users/kenny/anaconda3/envs/tensorflow-env/lib/python3.10/site-packages/tensorflow_text/core/pybinds/tflite_registrar.so\n  Expected in:     <B3F8A302-57B4-346D-A999-24F928D29E51> /Users/kenny/anaconda3/envs/tensorflow-env/lib/python3.10/site-packages/tensorflow/libtensorflow_framework.2.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m\n\u001b[1;32m      5\u001b[0m max_seq_length \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      6\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/tensorflow-env/lib/python3.10/site-packages/tensorflow_text/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_undocumented\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpybinds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tflite_registrar\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/kenny/anaconda3/envs/tensorflow-env/lib/python3.10/site-packages/tensorflow_text/core/pybinds/tflite_registrar.so, 0x0002): Symbol not found: __ZN4absl12lts_2023080210CHexEscapeENSt3__117basic_string_viewIcNS1_11char_traitsIcEEEE\n  Referenced from: <E5449F0F-CDE4-384F-B7F9-E7BD33907F26> /Users/kenny/anaconda3/envs/tensorflow-env/lib/python3.10/site-packages/tensorflow_text/core/pybinds/tflite_registrar.so\n  Expected in:     <B3F8A302-57B4-346D-A999-24F928D29E51> /Users/kenny/anaconda3/envs/tensorflow-env/lib/python3.10/site-packages/tensorflow/libtensorflow_framework.2.dylib"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "\n",
    "max_seq_length = 128\n",
    "epochs = 3\n",
    "batch_size = 32\n",
    "\n",
    "# Load the pre-trained BERT model from TensorFlow Hub\n",
    "bert_model = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/4\")\n",
    "\n",
    "# Define the model architecture for fine-tuning\n",
    "input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"input_mask\")\n",
    "segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "pooled_output, sequence_output = bert_model([input_word_ids, input_mask, segment_ids])\n",
    "\n",
    "# Add custom layers for classification\n",
    "# For example, you can add a Dense layer with softmax activation for multi-class classification\n",
    "num_classes = len(LABEL_LIST)\n",
    "output = tf.keras.layers.Dense(num_classes, activation='softmax')(pooled_output)\n",
    "\n",
    "# Create the model\n",
    "model = tf.keras.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=output)\n",
    "\n",
    "# Compile the model with appropriate loss function and optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Tokenize your input data and convert labels to one-hot encoding if necessary\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, validation_data=val_dataset, epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "# Evaluate the model\n",
    "model.evaluate(val_dataset)\n",
    "\n",
    "# Save the model\n",
    "model.save(\"bert_model\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
